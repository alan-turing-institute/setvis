{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce88b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from sklearn.metrics import mutual_info_score, adjusted_mutual_info_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294fb91",
   "metadata": {},
   "source": [
    "### For discussion\n",
    "\n",
    "- `information_gain` (cell below) and the scikit learn functions `mutual_info_score`, `adjusted_mutual_info_score` and `normalized_mutual_info_score` have the same interface, and called from `igr` interchangably\n",
    "\n",
    "- `information_gain` and `mutual_info_score` compute the same function; the others are different\n",
    "\n",
    "- ACE produces something different to all of these (on e.g. Nutrients.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15621c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(members, split):\n",
    "    '''\n",
    "    \n",
    "    *** THIS FUNCTION IS COPIED FROM https://stackoverflow.com/questions/46752650/information-gain-calculation-with-scikit-learn\n",
    "    \n",
    "    Measures the reduction in entropy after the split  \n",
    "    :param v: Pandas Series of the members\n",
    "    :param split:\n",
    "    :return:\n",
    "    '''\n",
    "    entropy_before = entropy(members.value_counts(normalize=True))\n",
    "    split.name = 'split'\n",
    "    members.name = 'members'\n",
    "    grouped_distrib = members.groupby(split) \\\n",
    "                        .value_counts(normalize=True) \\\n",
    "                        .reset_index(name='count') \\\n",
    "                        .pivot_table(index='split', columns='members', values='count').fillna(0) \n",
    "    entropy_after = entropy(grouped_distrib, axis=1)\n",
    "    entropy_after *= split.value_counts(sort=False, normalize=True)\n",
    "    return entropy_before - entropy_after.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def igr(df_features, target):\n",
    "    \"\"\"\n",
    "    Calculate the information gain ratio for each feature in a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_features : Dataframe\n",
    "        The features for which the information gain ratio will be calculated\n",
    "    target : Series\n",
    "        The targets for which the information gain ratio with each feature will be calculated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of feature names to information gain ratio, for each feature in df_features.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        col: normalized_mutual_info_score(df_features[col], target)\n",
    "        for col in df_features.columns\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
